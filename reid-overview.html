<!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>行人重识别Person Re-identification总结 - ChangingFond</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="ChangingFond"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="ChangingFond"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="Person re-identification, also known as person retrieval, is to match pedestrian images observed from non-overlapping camera views based on appearance.It receives increasing attentions in video surv"><meta property="og:type" content="blog"><meta property="og:title" content="行人重识别Person Re-identification总结"><meta property="og:url" content="http://blog.fcj.one/reid-overview.html"><meta property="og:site_name" content="ChangingFond"><meta property="og:description" content="Person re-identification, also known as person retrieval, is to match pedestrian images observed from non-overlapping camera views based on appearance.It receives increasing attentions in video surv"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="http://blog.fcj.one/img/og_image.png"><meta property="article:published_time" content="2018-07-14T01:10:22.000Z"><meta property="article:modified_time" content="2022-04-29T07:03:48.314Z"><meta property="article:author" content="ChangingFond"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Person Re-ID"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/og_image.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"http://blog.fcj.one/reid-overview.html"},"headline":"行人重识别Person Re-identification总结","image":["http://blog.fcj.one/img/og_image.png"],"datePublished":"2018-07-14T01:10:22.000Z","dateModified":"2022-04-29T07:03:48.314Z","author":{"@type":"Person","name":"ChangingFond"},"publisher":{"@type":"Organization","name":"ChangingFond","logo":{"@type":"ImageObject","url":"http://blog.fcj.one/img/avatar.png"}},"description":"Person re-identification, also known as person retrieval, is to match pedestrian images observed from non-overlapping camera views based on appearance.It receives increasing attentions in video surv"}</script><link rel="canonical" href="http://blog.fcj.one/reid-overview.html"><link rel="icon" href="/img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/github-gist.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script");e.src="//hm.baidu.com/hm.js?5c015a03c13b91e23ae1a852dd15b00d";var t=document.getElementsByTagName("script")[0];t.parentNode.insertBefore(e,t)}()</script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer="defer"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }
          Array
              .from(document.querySelectorAll('.tab-content'))
              .forEach($tab => {
                  $tab.classList.add('is-hidden');
              });
          Array
              .from(document.querySelectorAll('.tabs li'))
              .forEach($tab => {
                  $tab.classList.remove('is-active');
              });
          const $activeTab = document.querySelector(location.hash);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
          const $tabMenu = document.querySelector(`a[href="${location.hash}"]`);
          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><meta name="generator" content="Hexo 6.1.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/avatar.png" alt="ChangingFond" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">主页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ChangingFond"><i class="fab fa-github"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><article class="card-content article" role="article"><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time datetime="2018-07-14T01:10:22.000Z" title="7/14/2018, 1:10:22 AM">2018-07-14</time>发表</span><span class="level-item"><time datetime="2022-04-29T07:03:48.314Z" title="4/29/2022, 7:03:48 AM">2022-04-29</time>更新</span><span class="level-item"><a class="link-muted" href="/categories/%E8%A1%8C%E4%BA%BA%E9%87%8D%E8%AF%86%E5%88%AB/">行人重识别</a></span><span class="level-item">1 小时读完 (约8902字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">行人重识别Person Re-identification总结</h1><div class="content"><p>　　Person re-identification, also known as person retrieval, is to match pedestrian images observed from non-overlapping camera views based on appearance.It receives increasing attentions in video surveillance for its important applications in threat detection, human retrieval, and multi-camera tracking. It saves a lot of human labor in exhaustively searching for a person of interest from large amounts of video sequences.</p><span id="more"></span><p>Last Updated: Apr 26, 2019</p><h2 id="Table-of-Contents"><a href="#Table-of-Contents" class="headerlink" title="Table of Contents"></a><strong>Table of Contents</strong></h2><p><em>generated with <a target="_blank" rel="noopener" href="https://github.com/thlorenz/doctoc">DocToc</a></em></p><ul><li><a href="#Leaderboard">Leaderboard</a></li><li><a href="#Person-Re-identification-Person-Retrieval">Person Re-identification / Person Retrieval</a></li><li><a href="#Person-Search">Person Search</a></li><li><a href="#Pose-View-for-Re-ID">Pose/View for Re-ID</a></li><li><a href="#GAN-for-Re-ID">GAN for Re-ID</a></li><li><a href="#Human-Parsing-for-Re-ID">Human Parsing for Re-ID</a></li><li><a href="#Partial-Person-Re-ID">Partial Person Re-ID</a></li><li><a href="#RGB-IR-Re-ID">RGB-IR Re-ID</a></li><li><a href="#Depth-Based-Re-ID">Depth-Based Re-ID</a></li><li><a href="#Low-Resolution-Re-ID">Low Resolution Re-ID</a></li><li><a href="#Reinforcement-Learning-for-Re-ID">Reinforcement Learning for Re-ID</a></li><li><a href="#Attributes-Prediction-for-Re-ID">Attributes Prediction for Re-ID</a></li><li><a href="#Video-Person-Re-Identification">Video Person Re-Identification</a></li><li><a href="#Re-ranking">Re-ranking</a></li><li><a href="#Unsupervised-Re-ID">Unsupervised Re-ID</a></li><li><a href="#Weakly-Supervised-Person-Re-identification">Weakly Supervised Person Re-identification</a></li><li><a href="#Vehicle-Re-ID">Vehicle Re-ID</a></li><li><a href="#Deep-Metric-Learning">Deep Metric Learning</a></li><li><a href="#Projects">Projects</a></li><li><a href="#Evaluation">Evaluation</a></li><li><a href="#Datasets">Datasets</a></li><li><a href="#Tutorials">Tutorials</a></li><li><a href="#Experts">Experts</a></li><li><a href="#Resources">Resources</a></li></ul><h2 id="Leaderboard"><a href="#Leaderboard" class="headerlink" title="Leaderboard"></a>Leaderboard</h2><table><thead><tr><th style="text-align:center">Method</th><th style="text-align:center">backbone</th><th style="text-align:center">test size</th><th style="text-align:center">Market1501</th><th style="text-align:center">CUHK03 (detected)</th><th style="text-align:center">CUHK03 (detected/new)</th><th style="text-align:center">CUHK03 (labeled/new)</th><th style="text-align:center">CUHK-SYSU</th><th style="text-align:center">DukeMTMC-reID</th><th style="text-align:center">MARS</th></tr></thead><tbody><tr><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">rank1 / mAP</td><td style="text-align:center">rank1/ 5 / 10</td><td style="text-align:center">rank1 / mAP</td><td style="text-align:center">rank1 / mAP</td><td style="text-align:center">rank1 / mAP</td><td style="text-align:center"></td><td style="text-align:center">rank1 / mAP</td></tr><tr><td style="text-align:center">AlignedReID</td><td style="text-align:center">ResNet50-X</td><td style="text-align:center"></td><td style="text-align:center">92.6 / 82.3</td><td style="text-align:center">91.9 / 98.7 / 99.4</td><td style="text-align:center"></td><td style="text-align:center">86.8 / 79.1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">95.3 / 93.7</td></tr><tr><td style="text-align:center">AlignedReID (RK)</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">94.0 / 91.2</td><td style="text-align:center">96.1 / 99.5 / 99.6</td><td style="text-align:center"></td><td style="text-align:center">87.5 / 85.6</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">Deep-Person(SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">256×128</td><td style="text-align:center">92.31 / 79.58</td><td style="text-align:center">89.4 / 98.2 / 99.1</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">80.90 / 64.80</td></tr><tr><td style="text-align:center">Deep-Person(MQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">256×128</td><td style="text-align:center">94.48 / 85.09</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">PCB(SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">384x128</td><td style="text-align:center">92.4 / 77.3</td><td style="text-align:center"></td><td style="text-align:center">61.3 / 54.2</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">81.9 / 65.3</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">PCB+RPP(SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">384x128</td><td style="text-align:center">93.8 / 81.6</td><td style="text-align:center"></td><td style="text-align:center">63.7 / 57.5</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">83.3 / 69.2</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">PN-GAN (SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">89.43 / 72.58</td><td style="text-align:center">79.76/ 96.24/ 98.56</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">73.58 / 53.20</td></tr><tr><td style="text-align:center">PN-GAN (MQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">95.90 / 91.37</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MGN (SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">95.7 / 86.9</td><td style="text-align:center"></td><td style="text-align:center">66.8 / 66.0</td><td style="text-align:center">68.0 / 67.4</td><td style="text-align:center"></td><td style="text-align:center">88.7 / 78.4</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MGN (MQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">96.9 / 90.7</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MGN (SQ+RK)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">96.6 / 94.2</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">MGN (MQ+RK)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center"></td><td style="text-align:center">97.1 / 95.9</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center"></td></tr><tr><td style="text-align:center">HPM(SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">384x128</td><td style="text-align:center">94.2 / 82.7</td><td style="text-align:center"></td><td style="text-align:center">63.1 / 57.5</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">86.6 / 74.3</td><td style="text-align:center"></td></tr><tr><td style="text-align:center">HPM+HRE(SQ)</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">384x128</td><td style="text-align:center">93.9 / 83.1</td><td style="text-align:center"></td><td style="text-align:center">63.2 / 59.7</td><td style="text-align:center"></td><td style="text-align:center"></td><td style="text-align:center">86.3 / 74.5</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">SphereReID</td><td style="text-align:center">ResNet-50</td><td style="text-align:center">288×144</td><td style="text-align:center">94.4 / 83.6</td><td style="text-align:center">93.1 / 98.7 / 99.4</td><td style="text-align:center">63.2 / 59.7</td><td style="text-align:center"></td><td style="text-align:center">95.4 / 93.9</td><td style="text-align:center">83.9 / 68.5</td><td style="text-align:center">-</td></tr><tr><td style="text-align:center">Auto-ReID</td><td style="text-align:center"></td><td style="text-align:center">384x128</td><td style="text-align:center">94.5 / 85.1</td><td style="text-align:center"></td><td style="text-align:center">73.3 / 69.3</td><td style="text-align:center">77.9 / 73.0</td><td style="text-align:center"></td><td style="text-align:center">88.5 / 75.1</td><td style="text-align:center">-</td></tr></tbody></table><h2 id="Person-Re-identification-Person-Retrieval"><a href="#Person-Re-identification-Person-Retrieval" class="headerlink" title="Person Re-identification / Person Retrieval"></a>Person Re-identification / Person Retrieval</h2><p><strong>DeepReID: Deep Filter Pairing Neural Network for Person Re-Identification</strong><br>　　intro: CVPR 2014<br>　　paper: <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Li_DeepReID_Deep_Filter_2014_CVPR_paper.pdf</a></p><p><strong>An Improved Deep Learning Architecture for Person Re-Identification</strong><br>　　intro: CVPR 2015<br>　　paper: <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ahmed_An_Improved_Deep_2015_CVPR_paper.pdf</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID">https://github.com/Ning-Ding/Implementation-CVPR2015-CNN-for-ReID</a></p><p><strong>Deep Ranking for Person Re-identification via Joint Representation Learning</strong><br>　　intro: IEEE Transactions on Image Processing (TIP), 2016<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1505.06821">https://arxiv.org/abs/1505.06821</a></p><p><strong>PersonNet: Person Re-identification with Deep Convolutional Neural Networks</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1601.07255">http://arxiv.org/abs/1601.07255</a></p><p><strong>Learning Deep Feature Representations with Domain Guided Dropout for Person Re-identification</strong><br>　　intro: CVPR 2016<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.07528">https://arxiv.org/abs/1604.07528</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/Cysu/dgd_person_reid">https://github.com/Cysu/dgd_person_reid</a></p><p><strong>Person Re-Identification by Multi-Channel Parts-Based CNN with Improved Triplet Loss Function</strong><br>　　intro: CVPR 2016<br>　　paper: <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf">http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Cheng_Person_Re-Identification_by_CVPR_2016_paper.pdf</a></p><p><strong>Joint Learning of Single-image and Cross-image Representations for Person Re-identification</strong><br>　　intro: CVPR 2016<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2016/papers/Wang_Joint_Learning_of_CVPR_2016_paper.pdf</a></p><p><strong>End-to-End Comparative Attention Networks for Person Re-identification</strong><br>　　paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.04404">https://arxiv.org/abs/1606.04404</a></p><p><strong>A Multi-task Deep Network for Person Re-identification</strong><br>　　intro: AAAI 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1607.05369">http://arxiv.org/abs/1607.05369</a></p><p><strong>A Siamese Long Short-Term Memory Architecture for Human Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1607.08381">http://arxiv.org/abs/1607.08381</a></p><p><strong>Gated Siamese Convolutional Neural Network Architecture for Human Re-Identification</strong><br>　　intro: ECCV 2016<br>　　keywords: Market1501 rank1 = 65.9%<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1607.08378">https://arxiv.org/abs/1607.08378</a></p><p><strong>Deep Neural Networks with Inexact Matching for Person Re-Identification</strong><br>　　intro: NIPS 2016<br>　　keywords: Normalized correlation layer, CUHK03/CUHK01/QMULGRID<br>　　paper: <a target="_blank" rel="noopener" href="https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification">https://papers.nips.cc/paper/6367-deep-neural-networks-with-inexact-matching-for-person-re-identification</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/InnovArul/personreid_normxcorr">https://github.com/InnovArul/personreid_normxcorr</a></p><p><strong>Person Re-identification: Past, Present and Future</strong><br>　　paper: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.02984">https://arxiv.org/abs/1610.02984</a><br>　　note: <a target="_blank" rel="noopener" href="https://blog.csdn.net/zdh2010xyz/article/details/53741682">https://blog.csdn.net/zdh2010xyz/article/details/53741682</a></p><p><strong>Deep Learning Prototype Domains for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1610.05047">https://arxiv.org/abs/1610.05047</a></p><p><strong>Deep Transfer Learning for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05244">https://arxiv.org/abs/1611.05244</a><br>　　note: <a target="_blank" rel="noopener" href="https://blog.csdn.net/shenxiaolu1984/article/details/53607268">https://blog.csdn.net/shenxiaolu1984/article/details/53607268</a></p><p><strong>A Discriminatively Learned CNN Embedding for Person Re-identification</strong><br>　　intro: TOMM 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.05666">https://arxiv.org/abs/1611.05666</a><br>　　github(official, MatConvnet): <a target="_blank" rel="noopener" href="https://github.com/layumi/2016_person_re-ID">https://github.com/layumi/2016_person_re-ID</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/D-X-Y/caffe-reid">https://github.com/D-X-Y/caffe-reid</a></p><p><strong>Person Re-Identification via Recurrent Feature Aggregation</strong><br>　　intro: ECCV 2016<br>　　keywords: recurrent feature aggregation network (RFA-Net)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.06351">https://arxiv.org/abs/1701.06351</a><br>　　code: <a target="_blank" rel="noopener" href="https://sites.google.com/site/yanyichao91sjtu/">https://sites.google.com/site/yanyichao91sjtu/</a><br>　　github(official): <a target="_blank" rel="noopener" href="https://github.com/daodaofr/caffe-re-id">https://github.com/daodaofr/caffe-re-id</a></p><p><strong>Structured Deep Hashing with Convolutional Neural Networks for Fast Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1702.04179">https://arxiv.org/abs/1702.04179</a></p><p><strong>SVDNet for Pedestrian Retrieval</strong><br>　　intro: ICCV 2017 spotlight<br>　　intro: On the Market-1501 dataset, rank-1 accuracy is improved from 55.2% to 80.5% for CaffeNet,<br>and from 73.8% to 83.1% for ResNet-50<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.05693">https://arxiv.org/abs/1703.05693</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval">https://github.com/syfafterzy/SVDNet-for-Pedestrian-Retrieval</a></p><p><strong>In Defense of the Triplet Loss for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.07737">https://arxiv.org/abs/1703.07737</a><br>　　github(Theano): <a target="_blank" rel="noopener" href="https://github.com/VisualComputingInstitute/triplet-reid">https://github.com/VisualComputingInstitute/triplet-reid</a></p><p><strong>Beyond triplet loss: a deep quadruplet network for person re-identification</strong><br>　　intro: CVPR 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.01719">https://arxiv.org/abs/1704.01719</a><br>　　ppaper: <a target="_blank" rel="noopener" href="http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf">http://cvip.computing.dundee.ac.uk/papers/Chen_CVPR_2017_paper.pdf</a></p><p><strong>Quality Aware Network for Set to Set Recognition</strong><br>　　intro: CVPR 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1704.03373">https://arxiv.org/abs/1704.03373</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/sciencefans/Quality-Aware-Network">https://github.com/sciencefans/Quality-Aware-Network</a></p><p><strong>Learning Deep Context-aware Features over Body and Latent Parts for Person Re-identification</strong><br>　　intro: CVPR 2017. CASIA<br>　　keywords: Multi-Scale Context-Aware Network (MSCAN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.06555">https://arxiv.org/abs/1710.06555</a><br>　　supplemental: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/supplemental/Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf">Li_Learning_Deep_Context-Aware_2017_CVPR_supplemental.pdf</a></p><p><strong>Point to Set Similarity Based Deep Feature Learning for Person Re-identification</strong><br>　　intro: CVPR 2017<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhou_Point_to_Set_CVPR_2017_paper.pdf</a><br>　　github(stay tuned): <a target="_blank" rel="noopener" href="https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification">https://github.com/samaonline/Point-to-Set-Similarity-Based-Deep-Feature-Learning-for-Person-Re-identification</a></p><p><strong>Scalable Person Re-identification on Supervised Smoothed Manifold</strong><br>　　intro: CVPR 2017 spotlight<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.08359">https://arxiv.org/abs/1703.08359</a><br>　　youtube: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=bESdJgalQrg">https://www.youtube.com/watch?v=bESdJgalQrg</a></p><p><strong>Attention-based Natural Language Person Retrieval</strong><br>　　intro: CVPR 2017 Workshop (vision meets cognition)<br>　　keywords: Bidirectional Long Short　　Term Memory (BLSTM)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.08923">https://arxiv.org/abs/1705.08923</a></p><p><strong>Part-based Deep Hashing for Large-scale Person Re-identification</strong><br>　　intro: IEEE Transactions on Image Processing, 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.02145">https://arxiv.org/abs/1705.02145</a></p><p><strong>Deep Person Re-Identification with Improved Embedding and Efficient Training</strong><br>　　intro: IJCB 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.03332">https://arxiv.org/abs/1705.03332</a></p><p><strong>Towards a Principled Integration of Multi-Camera Re-Identification and Tracking through Optimal Bayes Filters</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.04608">https://arxiv.org/abs/1705.04608</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/VisualComputingInstitute/towards-reid-tracking">https://github.com/VisualComputingInstitute/towards-reid-tracking</a></p><p><strong>Person Re-Identification by Deep Joint Learning of Multi-Loss Classification</strong><br>　　intro: IJCAI 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.04724">https://arxiv.org/abs/1705.04724</a></p><p><strong>Deep Representation Learning with Part Loss for Person Re-Identification</strong><br>　　keywords: Part Loss Networks<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.00798">https://arxiv.org/abs/1707.00798</a></p><p><strong>Pedestrian Alignment Network for Large-scale Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.00408">https://arxiv.org/abs/1707.00408</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/layumi/Pedestrian_Alignment">https://github.com/layumi/Pedestrian_Alignment</a></p><p><strong>Learning Efficient Image Representation for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.02319">https://arxiv.org/abs/1707.02319</a></p><p><strong>Person Re-identification Using Visual Attention</strong><br>　　intro: ICIP 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07336">https://arxiv.org/abs/1707.07336</a></p><p><strong>What-and-Where to Match: Deep Spatially Multiplicative Integration Networks for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07074">https://arxiv.org/abs/1707.07074</a></p><p><strong>Deep Feature Learning via Structured Graph Laplacian Embedding for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07791">https://arxiv.org/abs/1707.07791</a></p><p><strong>Large Margin Learning in Set to Set Similarity Comparison for Person Re-identification</strong><br>　　intro: IEEE Transactions on Multimedia<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.05512">https://arxiv.org/abs/1708.05512</a></p><p><strong>Multi-scale Deep Learning Architectures for Person Re-identification</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.05165">https://arxiv.org/abs/1709.05165</a></p><p><strong>Person Re-Identification by Deep Learning Multi-Scale Representations</strong><br>　　intro: ICCV 2017<br>　　keywords: Deep Pyramid Feature Learning (DPFL)<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w37/Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf">Chen_Person_Re-Identification_by_ICCV_2017_paper.pdf</a><br>　　paper: <a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf">http://www.eecs.qmul.ac.uk/~sgg/papers/ChenEtAl_ICCV2017WK_CHI.pdf</a></p><p><strong>HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis</strong><br>　　intro: ICCV 2017. CUHK &amp; SenseTime,<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.09930">https://arxiv.org/abs/1709.09930</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/xh-liu/HydraPlus-Net">https://github.com/xh-liu/HydraPlus-Net</a></p><p><strong>Person Re-Identification with Vision and Language</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.01202">https://arxiv.org/abs/1710.01202</a></p><p><strong>Margin Sample Mining Loss: A Deep Learning Based Method for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1710.00478">https://arxiv.org/abs/1710.00478</a></p><p><strong>Pseudo-positive regularization for deep person re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.06500">https://arxiv.org/abs/1711.06500</a></p><p><strong>Let Features Decide for Themselves: Feature Mask Network for Person Re-identification</strong><br>　　keywords: Feature Mask Network (FMN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.07155">https://arxiv.org/abs/1711.07155</a></p><p><strong>AlignedReID: Surpassing Human-Level Performance in Person Re-Identification</strong><br>　　intro: Megvii Inc &amp; Zhejiang University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08184">https://arxiv.org/abs/1711.08184</a><br>　　evaluation website: (Market1501): <a target="_blank" rel="noopener" href="http://reid-challenge.megvii.com/">http://reid-challenge.megvii.com/</a><br>　　evaluation website: (CUHK03): <a target="_blank" rel="noopener" href="http://reid-challenge.megvii.com/cuhk03">http://reid-challenge.megvii.com/cuhk03</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch">https://github.com/huanghoujing/AlignedReID-Re-Production-Pytorch</a></p><p><strong>Region-based Quality Estimation Network for Large-scale Person Re-identification</strong><br>　　intro: AAAI 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08766">https://arxiv.org/abs/1711.08766</a></p><p><strong>Beyond Part Models: Person Retrieval with Refined Part Pooling</strong><br>　　keywords: Part-based Convolutional Baseline (PCB), Refined Part Pooling (RPP)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.09349">https://arxiv.org/abs/1711.09349</a></p><p><strong>Deep-Person: Learning Discriminative Deep Features for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.10658">https://arxiv.org/abs/1711.10658</a></p><p><strong>Hierarchical Cross Network for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.06820">https://arxiv.org/abs/1712.06820</a></p><p><strong>Re-ID done right: towards good practices for person re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.05339">https://arxiv.org/abs/1801.05339</a></p><p><strong>Triplet-based Deep Similarity Learning for Person Re-Identification</strong><br>　　intro: ICCV Workshops 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.03254">https://arxiv.org/abs/1802.03254</a></p><p><strong>Group Consistent Similarity Learning via Deep CRFs for Person Re-Identification</strong><br>　　intro: CVPR 2018 oral<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Group_Consistent_Similarity_CVPR_2018_paper.pdf</a></p><p><strong>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　keywords: similarity preserving generative adversarial network (SPGAN), Siamese network, CycleGAN, domain adaptation<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.07027">https://arxiv.org/abs/1711.07027</a></p><p><strong>Harmonious Attention Network for Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　keywords: Harmonious Attention CNN (HA-CNN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.08122">https://arxiv.org/abs/1802.08122</a></p><p><strong>Camera Style Adaptation for Person Re-identfication</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.10295">https://arxiv.org/abs/1711.10295</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/zhunzhong07/CamStyle">https://github.com/zhunzhong07/CamStyle</a></p><p><strong>Image-Image Domain Adaptation with Preserved Self-Similarity and Domain-Dissimilarity for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.07027">https://arxiv.org/abs/1711.07027</a></p><p><strong>Dual Attention Matching Network for Context-Aware Feature Sequence based Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09937">https://arxiv.org/abs/1803.09937</a></p><p><strong>Multi-Level Factorisation Net for Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　keywords: Multi-Level Factorisation Net (MLFN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09132">https://arxiv.org/abs/1803.09132</a></p><p><strong>Features for Multi-Target Multi-Camera Tracking and Re-Identification</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.10859">https://arxiv.org/abs/1803.10859</a></p><p><strong>Good Appearance Features for Multi-Target Multi-Camera Tracking</strong><br>　　intro: CVPR 2018 spotlight. Duke University<br>　　keywords: adaptive weighted triplet loss, hard-identity mining<br>　　project page: <a target="_blank" rel="noopener" href="http://vision.cs.duke.edu/DukeMTMC/">http://vision.cs.duke.edu/DukeMTMC/</a><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.10859">https://arxiv.org/abs/1803.10859</a></p><p><strong>Mask-guided Contrastive Attention Model for Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Song_Mask-Guided_Contrastive_Attention_CVPR_2018_paper.pdf</a></p><p><strong>Efficient and Deep Person Re-Identification using Multi-Level Similarity</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11353">https://arxiv.org/abs/1803.11353</a></p><p><strong>Person Re-identification with Cascaded Pairwise Convolutions</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Person_Re-Identification_With_CVPR_2018_paper.pdf</a></p><p><strong>Attention-Aware Compositional Network for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　intro: Sensets Technology Limited &amp; University of Sydney<br>　　keywords: Attention-Aware Compositional Network (AACN), Pose-guided Part Attention (PPA), Attention-aware Feature Composition (AFC)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.03344">https://arxiv.org/abs/1805.03344</a></p><p><strong>Deep Group-shuffling Random Walk for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_Deep_Group-Shuffling_Random_CVPR_2018_paper.pdf</a></p><p><strong>Adversarially Occluded Samples for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Huang_Adversarially_Occluded_Samples_CVPR_2018_paper.pdf</a></p><p><strong>Easy Identification from Better Constraints: Multi-Shot Person Re-Identification from Reference Constraints</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Easy_Identification_From_CVPR_2018_paper.pdf</a></p><p><strong>Eliminating Background-bias for Robust Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Tian_Eliminating_Background-Bias_for_CVPR_2018_paper.pdf</a></p><p><strong>End-to-End Deep Kronecker-Product Matching for Person Re-identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Shen_End-to-End_Deep_Kronecker-Product_CVPR_2018_paper.pdf</a></p><p><strong>Exploiting Transitivity for Learning Person Re-identification Models on a Budget</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Roy_Exploiting_Transitivity_for_CVPR_2018_paper.pdf</a></p><p><strong>Resource Aware Person Re-identification across Multiple Resolutions</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Resource_Aware_Person_CVPR_2018_paper.pdf</a></p><p><strong>Multi-Channel Pyramid Person Matching Network for Person Re-Identification</strong><br>　　intro: 32nd AAAI Conference on Artificial Intelligence<br>　　keywords: Multi-Channel deep convolutional Pyramid Person Matching Network (MC-PPMN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02558">https://arxiv.org/abs/1803.02558</a></p><p><strong>Pyramid Person Matching Network for Person Re-identification</strong><br>　　intro: 9th Asian Conference on Machine Learning (ACML2017) JMLR Workshop and Conference Proceedings<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.02547">https://arxiv.org/abs/1803.02547</a></p><p><strong>Virtual CNN Branching: Efficient Feature Ensemble for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.05872">https://arxiv.org/abs/1803.05872</a></p><p><strong>Adversarial Binary Coding for Efficient Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.10914">https://arxiv.org/abs/1803.10914</a></p><p><strong>Learning View-Specific Deep Networks for Person Re-Identification</strong><br>　　intro: IEEE Transactions on image processing. Sun Yat-Sen University<br>　　keywords: cross-view Euclidean constraint (CV-EC), cross-view center loss (CV-CL)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.11333">https://arxiv.org/abs/1803.11333</a></p><p><strong>Learning Discriminative Features with Multiple Granularities for Person Re-Identification</strong><br>　　intro: Shanghai Jiao Tong University &amp; CloudWalk<br>　　keywords: Multiple Granularity Network (MGN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.01438">https://arxiv.org/abs/1804.01438</a></p><p><strong>Recurrent Neural Networks for Person Re-identification Revisited</strong><br>　　intro: Stanford University &amp; Google AI<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.03281">https://arxiv.org/abs/1804.03281</a></p><p><strong>MaskReID: A Mask Based Deep Ranking Neural Network for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.03864">https://arxiv.org/abs/1804.03864</a></p><p><strong>Horizontal Pyramid Matching for Person Re-identification</strong><br>　　intro: AAAI 2019<br>　　intro: UIUC &amp; IBM Research &amp; Cornell University &amp; Stevens Institute of Technology &amp;CloudWalk Technology<br>　　keywords: Horizontal Pyramid Matching (HPM), Horizontal Pyramid Pooling (HPP), horizontal random erasing (HRE)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.05275">https://arxiv.org/abs/1804.05275</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/OasisYang/HPM">https://github.com/OasisYang/HPM</a></p><p><strong>Part-Aligned Bilinear Representations for Person Re-identification</strong><br>　　intro: Seoul National University &amp; Microsoft Research &amp; Max Planck Institute &amp; University of Tubingen &amp; JD.COM<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.07094">https://arxiv.org/abs/1804.07094</a></p><p><strong>Deep Co-attention based Comparators For Relative Representation Learning in Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.11027">https://arxiv.org/abs/1804.11027</a></p><p><strong>Feature Affinity based Pseudo Labeling for Semi-supervised Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.06118">https://arxiv.org/abs/1805.06118</a></p><p><strong>Resource Aware Person Re-identification across Multiple Resolutions</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.08805">https://arxiv.org/abs/1805.08805</a></p><p><strong>Semantically Selective Augmentation for Deep Compact Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.04074">https://arxiv.org/abs/1806.04074</a></p><p><strong>SphereReID: Deep Hypersphere Manifold Embedding for Person Re-Identification</strong><br>　　intro: it achieves 94.4% rank-1 accuracy on Market-1501 and 83.9% rank-1 accuracy on DukeMTMC-reID<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.00537">https://arxiv.org/abs/1807.00537</a></p><p><strong>Multi-task Mid-level Feature Alignment Network for Unsupervised Cross-Dataset Person Re-Identification</strong><br>　　intro: BMVC 2018. University of Warwick &amp; Nanyang Technological University &amp; Charles Sturt University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.01440">https://arxiv.org/abs/1807.01440</a></p><p><strong>Discriminative Feature Learning with Foreground Attention for Person Re-Identification</strong><br> arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.01455">https://arxiv.org/abs/1807.01455</a></p><p><strong>Part-Aligned Bilinear Representations for Person Re-identification</strong><br>　　intro: ECCV 2018<br>　　intro: Seoul National University &amp; Microsoft Research &amp; Max Planck Institute &amp; University of Tubingen &amp; JD.COM<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.07094">https://arxiv.org/abs/1804.07094</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/yuminsuh/part_bilinear_reid">https://github.com/yuminsuh/part_bilinear_reid</a></p><p><strong>Mancs: A Multi-task Attentional Network with Curriculum Sampling for Person Re-identification</strong><br>　　intro: ECCV 2018. Huazhong University of Science and Technology &amp; Horizon Robotics Inc.</p><p><strong>Improving Deep Visual Representation for Person Re-identification by Global and Local Image-language Association</strong><br>　　intro: ECCV 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.01571">https://arxiv.org/abs/1808.01571</a></p><p><strong>Deep Sequential Multi-camera Feature Fusion for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.07295">https://arxiv.org/abs/1807.07295</a></p><p><strong>Improving Deep Models of Person Re-identification for Cross-Dataset Usage</strong><br>　　intro: AIAI 2018 (14th International Conference on Artificial Intelligence Applications and Innovations) proceeding<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.08526">https://arxiv.org/abs/1807.08526</a></p><p><strong>Measuring the Temporal Behavior of Real-World Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.05499">https://arxiv.org/abs/1808.05499</a></p><p><strong>Alignedreid＋+: Dynamically Matching Local Information for Person Re-Identification</strong><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/michuanhaohao/AlignedReID">https://github.com/michuanhaohao/AlignedReID</a></p><p><strong>Sparse Label Smoothing for Semi-supervised Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.04976">https://arxiv.org/abs/1809.04976</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/jpainam/SLS_ReID">https://github.com/jpainam/SLS_ReID</a></p><p><strong>In Defense of the Classification Loss for Person Re-Identification</strong><br>　　intro: University of Science and Technology of China &amp; Microsoft Research Asia<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.05864">https://arxiv.org/abs/1809.05864</a></p><p><strong>FD-GAN: Pose-guided Feature Distilling GAN for Robust Person Re-identification</strong><br>　　intro: NIPS 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.02936">https://arxiv.org/abs/1810.02936</a><br>　　github(Pytorch, official): <a target="_blank" rel="noopener" href="https://github.com/yxgeee/FD-GAN">https://github.com/yxgeee/FD-GAN</a></p><p><strong>Image-to-Video Person Re-Identification by Reusing Cross-modal Embeddings</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.03989">https://arxiv.org/abs/1810.03989</a></p><p><strong>Attention Driven Person Re-identification</strong><br>　　intro: Pattern Recognition (PR)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.05866">https://arxiv.org/abs/1810.05866</a></p><p><strong>A Coarse-to-fine Pyramidal Model for Person Re-identification via Multi-Loss Dynamic Training</strong><br>　　intro: YouTu Lab, Tencent<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.12193">https://arxiv.org/abs/1810.12193</a></p><p><strong>M2M-GAN: Many-to-Many Generative Adversarial Transfer Learning for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.03768">https://arxiv.org/abs/1811.03768</a></p><p><strong>Batch Feature Erasing for Person Re-identification and Beyond</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07130">https://arxiv.org/abs/1811.07130</a><br>　　github(official, Pytorch): <a target="_blank" rel="noopener" href="https://github.com/daizuozhuo/batch-feature-erasing-network">https://github.com/daizuozhuo/batch-feature-erasing-network</a></p><p><strong>Re-Identification with Consistent Attentive Siamese Networks</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07487">https://arxiv.org/abs/1811.07487</a></p><p><strong>One Shot Domain Adaptation for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.10144">https://arxiv.org/abs/1811.10144</a></p><p><strong>Parameter-Free Spatial Attention Network for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.12150">https://arxiv.org/abs/1811.12150</a></p><p><strong>Spectral Feature Transformation for Person Re-identification</strong><br>　　intro: University of Chinese Academy of Sciences &amp; TuSimple<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11405">https://arxiv.org/abs/1811.11405</a></p><p><strong>Identity Preserving Generative Adversarial Network for Cross-Domain Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.11510">https://arxiv.org/abs/1811.11510</a></p><p><strong>Dissecting Person Re-identification from the Viewpoint of Viewpoint</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02162">https://arxiv.org/abs/1812.02162</a></p><p><strong>Fast and Accurate Person Re-Identification with RMNet</strong><br>　　intro: IOTG Computer Vision (ICV), Intel<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.02465">https://arxiv.org/abs/1812.02465</a></p><p><strong>Spatial-Temporal Person Re-identification</strong><br>　　intro: AAAI 2019<br>　　intro: Sun Yat-sen University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.03282">https://arxiv.org/abs/1812.03282</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/Wanggcong/Spatial-Temporal-Re-identification">https://github.com/Wanggcong/Spatial-Temporal-Re-identification</a></p><p><strong>Omni-directional Feature Learning for Person Re-identification</strong><br>　　intro: Tongji University<br>　　keywords: OIM loss<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05319">https://arxiv.org/abs/1812.05319</a></p><p><strong>Learning Incremental Triplet Margin for Person Re-identification</strong><br>　　intro: AAAI 2019 spotlight<br>　　intro: Hikvision Research Institute<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.06576">https://arxiv.org/abs/1812.06576</a></p><p><strong>Densely Semantically Aligned Person Re-Identification</strong><br>　　intro: USTC &amp; MSRA<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.08967">https://arxiv.org/abs/1812.08967</a></p><p><strong>EANet: Enhancing Alignment for Cross-Domain Person Re-identification</strong><br>　　intro: CRISE &amp; CASIA &amp; Horizon Robotics<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.11369">https://arxiv.org/abs/1812.11369</a><br>　　github(official, Pytorch): <a target="_blank" rel="noopener" href="https://github.com/huanghoujing/EANet">https://github.com/huanghoujing/EANet</a><br>　　blog: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/53660395">https://zhuanlan.zhihu.com/p/53660395</a></p><p><strong>Backbone Can Not be Trained at Once: Rolling Back to Pre-trained Network for Person Re-Identification</strong><br>　　intro: AAAI 2019<br>　　intro: Seoul National University &amp; Samsung SDS<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.06140">https://arxiv.org/abs/1901.06140</a></p><p><strong>Ensemble Feature for Person Re-Identification</strong><br>　　keywords: EnsembleNet<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.05798">https://arxiv.org/abs/1901.05798</a></p><p><strong>Adversarial Metric Attack for Person Re-identification</strong><br>　　intro: University of Oxford &amp; Johns Hopkins University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.10650">https://arxiv.org/abs/1901.10650</a></p><p><strong>Discovering Underlying Person Structure Pattern with Relative Local Distance for Person Re-identification</strong><br>　　intro: SYSU<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.10100">https://arxiv.org/abs/1901.10100</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/Wanggcong/RLD_codes">https://github.com/Wanggcong/RLD_codes</a></p><p><strong>Attributes-aided Part Detection and Refinement for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.10528">https://arxiv.org/abs/1902.10528</a></p><p><strong>Bags of Tricks and A Strong Baseline for Deep Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07071">https://arxiv.org/abs/1903.07071</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/michuanhaohao/reid-strong-baseline">https://github.com/michuanhaohao/reid-strong-baseline</a></p><p><strong>Auto-ReID: Searching for a Part-aware ConvNet for Person Re-Identification</strong><br>　　keywords: NAS<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.09776">https://arxiv.org/abs/1903.09776</a></p><p><strong>Perceive Where to Focus: Learning Visibility-aware Part-level Features for Partial Person Re-identification</strong><br>　　intro: CVPR 2019<br>　　intro: Tsinghua University &amp; Megvii Technology<br>　　keywords: Visibility-aware Part Model (VPM)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00537">https://arxiv.org/abs/1904.00537</a></p><p><strong>Pedestrian re-identification based on Tree branch network with local and global learning</strong><br>　　intro: ICME 2019 oral<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00355">https://arxiv.org/abs/1904.00355</a></p><p><strong>Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification</strong><br>　　intro: CVPR 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01990">https://arxiv.org/abs/1904.01990</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/zhunzhong07/ECN">https://github.com/zhunzhong07/ECN</a></p><p><strong>Person Re-identification with Bias-controlled Adversarial Training</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.00244">https://arxiv.org/abs/1904.00244</a></p><p><strong>Person Re-identification with Metric Learning using Privileged Information</strong><br>　　intro: IEEE TIP<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.05005">https://arxiv.org/abs/1904.05005</a></p><p><strong>Joint Discriminative and Generative Learning for Person Re-identification</strong><br>　　intro: CVPR 2019 oral<br>　　intro: NVIDIA &amp; University of Technology Sydney &amp; Australian National University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.07223">https://arxiv.org/abs/1904.07223</a></p><h2 id="Person-Search"><a href="#Person-Search" class="headerlink" title="Person Search"></a>Person Search</h2><p><strong>Joint Detection and Identification Feature Learning for Person Search</strong><br>　　intro: CVPR 2017 Spotlight<br>　　keywords: Online Instance Matching (OIM) loss function<br>　　homepage(dataset+code):<a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html">http://www.ee.cuhk.edu.hk/~xgwang/PS/dataset.html</a><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.01850">https://arxiv.org/abs/1604.01850</a><br>　　paper: <a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf">http://www.ee.cuhk.edu.hk/~xgwang/PS/paper.pdf</a><br>　　github(official. Caffe): <a target="_blank" rel="noopener" href="https://github.com/ShuangLI59/person_search">https://github.com/ShuangLI59/person_search</a></p><p><strong>Person Re-identification in the Wild</strong><br>　　intro: CVPR 2017 spotlight<br>　　keywords: PRW dataset<br>　　project page: <a target="_blank" rel="noopener" href="http://www.liangzheng.com.cn/Project/project_prw.html">http://www.liangzheng.com.cn/Project/project_prw.html</a><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1604.02531">https://arxiv.org/abs/1604.02531</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/liangzheng06/PRW-baseline">https://github.com/liangzheng06/PRW-baseline</a><br>　　youtube: <a target="_blank" rel="noopener" href="https://www.youtube.com/watch?v=dbOGwBITJqo">https://www.youtube.com/watch?v=dbOGwBITJqo</a></p><p><strong>IAN: The Individual Aggregation Network for Person Search</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.05552">https://arxiv.org/abs/1705.05552</a></p><p><strong>Neural Person Search Machines</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.06777">https://arxiv.org/abs/1707.06777</a></p><p><strong>End-to-End Detection and Re-identification Integrated Net for Person Search</strong><br>　　keywords: I-Net<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.00376">https://arxiv.org/abs/1804.00376</a></p><p><strong>Person Search via A Mask-guided Two-stream CNN Model</strong><br>　　intro: ECCV 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.08107">https://arxiv.org/abs/1807.08107</a></p><p><strong>Person Search by Multi-Scale Matching</strong><br>　　intro: ECCV 2018<br>　　keywords: Cross-Level Semantic Alignment (CLSA)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.08582">https://arxiv.org/abs/1807.08582</a></p><p><strong>Learning Context Graph for Person Search</strong><br>　　intro: CVPR 2019<br>　　intro: Shanghai Jiao Tong University &amp; Tencent YouTu Lab &amp; Inception Institute of Artificial Intelligence, UAE<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01830">https://arxiv.org/abs/1904.01830</a></p><h2 id="Pose-View-for-Re-ID"><a href="#Pose-View-for-Re-ID" class="headerlink" title="Pose/View for Re-ID"></a>Pose/View for Re-ID</h2><p><strong>Pose Invariant Embedding for Deep Person Re-identification</strong><br>　　keywords: pose invariant embedding (PIE), PoseBox fusion (PBF) CNN<br>　　arixv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.07732">https://arxiv.org/abs/1701.07732</a></p><p><strong>Deeply-Learned Part-Aligned Representations for Person Re-Identification</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.07256">https://arxiv.org/abs/1707.07256</a><br>　　github(official, Caffe): <a target="_blank" rel="noopener" href="https://github.com/zlmzju/part_reid">https://github.com/zlmzju/part_reid</a></p><p><strong>Spindle Net: Person Re-identification with Human Body Region Guided Feature Decomposition and Fusion</strong><br>　　intro: CVPR 2017<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhao_Spindle_Net_Person_CVPR_2017_paper.pdf</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/yokattame/SpindleNet">https://github.com/yokattame/SpindleNet</a></p><p><strong>Pose-driven Deep Convolutional Model for Person Re-identification</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.08325">https://arxiv.org/abs/1709.08325</a></p><p><strong>A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.10378">https://arxiv.org/abs/1711.10378</a><br>　　github(official): <a target="_blank" rel="noopener" href="https://github.com/pse-ecn/pose-sensitive-embedding">https://github.com/pse-ecn/pose-sensitive-embedding</a></p><p><strong>Pose-Driven Deep Models for Person Re-Identification</strong><br>　　intro: Masters thesis<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.08709">https://arxiv.org/abs/1803.08709</a></p><p><strong>Pose Transferrable Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_Pose_Transferrable_Person_CVPR_2018_paper.pdf</a></p><p><strong>Person re-identification with fusion of hand-crafted and deep pose-based body region features</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.10630">https://arxiv.org/abs/1803.10630</a></p><h2 id="GAN-for-Re-ID"><a href="#GAN-for-Re-ID" class="headerlink" title="GAN for Re-ID"></a>GAN for Re-ID</h2><p><strong>Unlabeled Samples Generated by GAN Improve the Person Re-identification Baseline in vitro</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.07717">https://arxiv.org/abs/1701.07717</a><br>　　github(official, Matlab): <a target="_blank" rel="noopener" href="https://github.com/layumi/Person-reID_GAN">https://github.com/layumi/Person-reID_GAN</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/qiaoguan/Person-reid-GAN-pytorch">https://github.com/qiaoguan/Person-reid-GAN-pytorch</a></p><p><strong>Person Transfer GAN to Bridge Domain Gap for Person Re-Identification</strong><br>　　intro: CVPR 2018 spotlight<br>　　intro: PTGAN<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.08565">https://arxiv.org/abs/1711.08565</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/JoinWei-PKU/PTGAN">https://github.com/JoinWei-PKU/PTGAN</a></p><p><strong>Pose-Normalized Image Generation for Person Re-identification</strong><br>　　keywords: PN-GAN<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.02225">https://arxiv.org/abs/1712.02225</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/naiq/PN_GAN">https://github.com/naiq/PN_GAN</a></p><p><strong>Multi-pseudo Regularized Label for Generated Samples in Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.06742">https://arxiv.org/abs/1801.06742</a></p><h2 id="Human-Parsing-for-Re-ID"><a href="#Human-Parsing-for-Re-ID" class="headerlink" title="Human Parsing for Re-ID"></a>Human Parsing for Re-ID</h2><p><strong>Human Semantic Parsing for Person Re-identification</strong><br>　　intro: CVPR 2018. SPReID<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.00216">https://arxiv.org/abs/1804.00216</a></p><p><strong>Improved Person Re-Identification Based on Saliency and Semantic Parsing with Deep Neural Network Models</strong><br>　　keywords: Saliency-Semantic Parsing Re-Identification (SSP-ReID)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05618">https://arxiv.org/abs/1807.05618</a></p><h2 id="Partial-Person-Re-ID"><a href="#Partial-Person-Re-ID" class="headerlink" title="Partial Person Re-ID"></a>Partial Person Re-ID</h2><p><strong>Partial Person Re-identification</strong><br>　　intro: ICCV 2015<br>　　arxiv: <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Zheng_Partial_Person_Re-Identification_ICCV_2015_paper.pdf</a></p><p><strong>Deep Spatial Feature Reconstruction for Partial Person Re-identification: Alignment-Free Approach</strong><br>　　intro: CVPR 2018.<br>　　keywords: Market1501 rank1=83.58%<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1801.00881">https://arxiv.org/abs/1801.00881</a></p><p><strong>Occluded Person Re-identification</strong><br>　　intro: ICME 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.02792">https://arxiv.org/abs/1804.02792</a></p><p><strong>Partial Person Re-identification with Alignment and Hallucination</strong><br>　　intro: Imperial College London<br>　　keywords: Partial Matching Net (PMN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.09162">https://arxiv.org/abs/1807.09162</a></p><p><strong>SCPNet: Spatial-Channel Parallelism Network for Joint Holistic and Partial Person Re-Identification</strong><br>　　intro: ACCV 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.06996">https://arxiv.org/abs/1810.06996</a></p><p><strong>STNReID : Deep Convolutional Networks with Pairwise Spatial Transformer Networks for Partial Person Re-identification</strong><br>　　intro: Zhejiang University &amp; Megvii Inc<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.07072">https://arxiv.org/abs/1903.07072</a></p><p><strong>Foreground-aware Pyramid Reconstruction for Alignment-free Occluded Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04975">https://arxiv.org/abs/1904.04975</a></p><h2 id="RGB-IR-Re-ID"><a href="#RGB-IR-Re-ID" class="headerlink" title="RGB-IR Re-ID"></a>RGB-IR Re-ID</h2><p><strong>RGB-Infrared Cross-Modality Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf">Wu_RGB-Infrared_Cross-Modality_Person_ICCV_2017_paper.pdf</a></p><h2 id="Depth-Based-Re-ID"><a href="#Depth-Based-Re-ID" class="headerlink" title="Depth-Based Re-ID"></a>Depth-Based Re-ID</h2><p><strong>Reinforced Temporal Attention and Split-Rate Transfer for Depth-Based Person Re-Identification</strong><br>　　intro: ECCV 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf">Nikolaos_Karianakis_Reinforced_Temporal_Attention_ECCV_2018_paper.pdf</a></p><p><strong>A Cross-Modal Distillation Network for Person Re-identification in RGB-Depth</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.11641">https://arxiv.org/abs/1810.11641</a></p><h2 id="Low-Resolution-Re-ID"><a href="#Low-Resolution-Re-ID" class="headerlink" title="Low Resolution Re-ID"></a>Low Resolution Re-ID</h2><p><strong>Multi-scale Learning for Low-resolution Person Re-identification</strong><br>　　intro: ICCV 2015<br>　　arxiv: <a target="_blank" rel="noopener" href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Li_Multi-Scale_Learning_for_ICCV_2015_paper.pdf</a></p><p><strong>Cascaded SR-GAN for Scale-Adaptive Low Resolution Person Re-identification</strong><br>　　intro: IJCAI 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://www.ijcai.org/proceedings/2018/0541.pdf">https://www.ijcai.org/proceedings/2018/0541.pdf</a></p><p><strong>Deep Low-Resolution Person Re-Identification</strong><br>　　intro: AAAI 2018<br>　　keywords: Super resolution and Identity joiNt learninG (SING)<br>　　paper: <a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf">http://www.eecs.qmul.ac.uk/~xiatian/papers/JiaoEtAl_2018AAAI.pdf</a></p><h2 id="Reinforcement-Learning-for-Re-ID"><a href="#Reinforcement-Learning-for-Re-ID" class="headerlink" title="Reinforcement Learning for Re-ID"></a>Reinforcement Learning for Re-ID</h2><p><strong>Deep Reinforcement Learning Attention Selection for Person Re-Identification</strong><br><strong>Identity Alignment by Noisy Pixel Removal</strong><br>　　intro: BMVC 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.02785">https://arxiv.org/abs/1707.02785</a><br>　　paper: <a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf">http://www.eecs.qmul.ac.uk/~sgg/papers/LanEtAl_2017BMVC.pdf</a></p><h2 id="Attributes-Prediction-for-Re-ID"><a href="#Attributes-Prediction-for-Re-ID" class="headerlink" title="Attributes Prediction for Re-ID"></a>Attributes Prediction for Re-ID</h2><p><strong>Multi-Task Learning with Low Rank Attribute Embedding for Person Re-identification</strong><br>　　intro: ICCV 2015<br>　　paper: <a target="_blank" rel="noopener" href="http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf">http://legacydirs.umiacs.umd.edu/~fyang/papers/iccv15.pdf</a></p><p><strong>Deep Attributes Driven Multi-Camera Person Re-identification</strong><br>　　intro: ECCV 2016<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1605.03259">https://arxiv.org/abs/1605.03259</a></p><p><strong>Improving Person Re-identification by Attribute and Identity Learning</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1703.07220">https://arxiv.org/abs/1703.07220</a></p><p><strong>Person Re-identification by Deep Learning Attribute-Complementary Information</strong><br>　　intro: CVPR 2017 workshop<br>　　paper: <a target="_blank" rel="noopener" href="https://sci-hub.tw/10.1109/CVPRW.2017.186">https://sci-hub.tw/10.1109/CVPRW.2017.186</a></p><p><strong>CA3Net: Contextual-Attentional Attribute-Appearance Network for Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07544">https://arxiv.org/abs/1811.07544</a></p><h2 id="Video-Person-Re-Identification"><a href="#Video-Person-Re-Identification" class="headerlink" title="Video Person Re-Identification"></a>Video Person Re-Identification</h2><p><strong>Recurrent Convolutional Network for Video-based Person Re-Identification</strong><br>　　intro: CVPR 2016<br>　　paper: <a target="_blank" rel="noopener" href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf">McLaughlin_Recurrent_Convolutional_Network_CVPR_2016_paper.pdf</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID">https://github.com/niallmcl/Recurrent-Convolutional-Video-ReID</a></p><p><strong>Deep Recurrent Convolutional Networks for Video-based Person Re-identification: An End-to-End Approach</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1606.01609">https://arxiv.org/abs/1606.01609</a></p><p><strong>Jointly Attentive Spatial-Temporal Pooling Networks for Video-based Person Re-Identification</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.02286">https://arxiv.org/abs/1708.02286</a></p><p><strong>Three-Stream Convolutional Networks for Video-based Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.01652">https://arxiv.org/abs/1712.01652</a></p><p><strong>LVreID: Person Re-Identification with Long Sequence Videos</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07286">https://arxiv.org/abs/1712.07286</a></p><p><strong>Multi-shot Pedestrian Re-identification via Sequential Decision Making</strong><br>　　intro: CVPR 2018. TuSimple<br>　　keywords: reinforcement learning<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07257">https://arxiv.org/abs/1712.07257</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/TuSimple/rl-multishot-reid">https://github.com/TuSimple/rl-multishot-reid</a></p><p><strong>LVreID: Person Re-Identification with Long Sequence Videos</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1712.07286">https://arxiv.org/abs/1712.07286</a></p><p><strong>Diversity Regularized Spatiotemporal Attention for Video-based Person Re-identification</strong><br>　　intro: CUHK-SenseTime &amp; Argo AI<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09882">https://arxiv.org/abs/1803.09882</a></p><p><strong>Video Person Re-identification with Competitive Snippet-similarity Aggregation and Co-attentive Snippet Embedding</strong><br>　　intro: CVPR 2018 Poster<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_Video_Person_Re-Identification_CVPR_2018_paper.pdf</a></p><p><strong>Exploit the Unknown Gradually: One-Shot Video-Based Person Re-Identification by Stepwise Learning</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Exploit_the_Unknown_CVPR_2018_paper.pdf</a></p><p><strong>Revisiting Temporal Modeling for Video-based Person ReID</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1805.02104">https://arxiv.org/abs/1805.02104</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/jiyanggao/Video-Person-ReID">https://github.com/jiyanggao/Video-Person-ReID</a></p><p><strong>Video Person Re-identification by Temporal Residual Learning</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1802.07918">https://arxiv.org/abs/1802.07918</a></p><p><strong>A Spatial and Temporal Features Mixture Model with Body Parts for Video-based Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.00975">https://arxiv.org/abs/1807.00975</a></p><p><strong>Video-based Person Re-identification via 3D Convolutional Networks and Non-local Attention</strong><br>　　intro: University of Science and Technology of China &amp; University of Chinese Academy of Sciences<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05073">https://arxiv.org/abs/1807.05073</a></p><p><strong>Spatial-Temporal Synergic Residual Learning for Video Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05799">https://arxiv.org/abs/1807.05799</a></p><p><strong>Where-and-When to Look: Deep Siamese Attention Networks for Video-based Person Re-identification</strong><br>　　intro: IEEE Transactions on Multimedia<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.01911">https://arxiv.org/abs/1808.01911</a></p><p><strong>STA: Spatial-Temporal Attention for Large-Scale Video-based Person Re-Identification</strong><br>　　intro: AAAI 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.04129">https://arxiv.org/abs/1811.04129</a></p><p><strong>Multi-scale 3D Convolution Network for Video Based Person Re-Identification</strong><br>　　intro: AAAI 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.07468">https://arxiv.org/abs/1811.07468</a></p><p><strong>Deep Active Learning for Video-based Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.05785">https://arxiv.org/abs/1812.05785</a></p><p><strong>Spatial and Temporal Mutual Promotion for Video-based Person Re-identification</strong><br>　　intro: AAAI 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10305">https://arxiv.org/abs/1812.10305</a></p><p><strong>3D PersonVLAD: Learning Deep Global Representations for Video-based Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.10222">https://arxiv.org/abs/1812.10222</a></p><p><strong>SCAN: Self-and-Collaborative Attention Network for Video Person Re-identification</strong><br>　　intro: TIP 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1807.05688">https://arxiv.org/abs/1807.05688</a></p><p><strong>GAN-based Pose-aware Regulation for Video-based Person Re-identification</strong><br>　　intro: Heriot-Watt University &amp; University of Edinburgh &amp; Queen’s University Belfast &amp; Anyvision<br>　　keywords: Weighted Fusion (WF) &amp; Weighted-Pose Regulation (WPR)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.11552">https://arxiv.org/abs/1903.11552</a></p><p><strong>Convolutional Temporal Attention Model for Video-based Person Re-identification</strong><br>　　intro: ICME 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.04492">https://arxiv.org/abs/1904.04492</a></p><h2 id="Re-ranking"><a href="#Re-ranking" class="headerlink" title="Re-ranking"></a>Re-ranking</h2><p><strong>Divide and Fuse: A Re-ranking Approach for Person Re-identification</strong><br>　　intro: BMVC 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.04169">https://arxiv.org/abs/1708.04169</a></p><p><strong>Re-ranking Person Re-identification with k-reciprocal Encoding</strong><br>　　intro: CVPR 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1701.08398">https://arxiv.org/abs/1701.08398</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/zhunzhong07/person-re-ranking">https://github.com/zhunzhong07/person-re-ranking</a></p><p><strong>A Pose-Sensitive Embedding for Person Re-Identification with Expanded Cross Neighborhood Re-Ranking</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.10378">https://arxiv.org/abs/1711.10378</a><br>　　github(official): <a target="_blank" rel="noopener" href="https://github.com/pse-ecn/expanded-cross-neighborhood">https://github.com/pse-ecn/expanded-cross-neighborhood</a></p><p><strong>Adaptive Re-ranking of Deep Feature for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.08561">https://arxiv.org/abs/1811.08561</a></p><h2 id="Unsupervised-Re-ID"><a href="#Unsupervised-Re-ID" class="headerlink" title="Unsupervised Re-ID"></a>Unsupervised Re-ID</h2><p><strong>Unsupervised Person Re-identification: Clustering and Fine-tuning</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1705.10444">https://arxiv.org/abs/1705.10444</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning">https://github.com/hehefan/Unsupervised-Person-Re-identification-Clustering-and-Fine-tuning</a></p><p><strong>Stepwise Metric Promotion for Unsupervised Video Person Re-identification</strong><br>　　intro: ICCV 2017<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf">http://openaccess.thecvf.com/content_ICCV_2017/papers/Liu_Stepwise_Metric_Promotion_ICCV_2017_paper.pdf</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/lilithliu/StepwiseMetricPromotion-code">https://github.com/lilithliu/StepwiseMetricPromotion-code</a></p><p><strong>Dynamic Label Graph Matching for Unsupervised Video Re-Identification</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1709.09297">https://arxiv.org/abs/1709.09297</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/mangye16/dgm_re-id">https://github.com/mangye16/dgm_re-id</a></p><p><strong>Unsupervised Cross-dataset Person Re-identification by Transfer Learning of Spatio-temporal Patterns</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.07293">https://arxiv.org/abs/1803.07293</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/ahangchen/TFusion">https://github.com/ahangchen/TFusion</a><br>　　blog: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34778414">https://zhuanlan.zhihu.com/p/34778414</a></p><p><strong>Cross-dataset Person Re-Identification Using Similarity Preserved Generative Adversarial Networks</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.04533">https://arxiv.org/abs/1806.04533</a></p><p><strong>Transferable Joint Attribute-Identity Deep Learning for Unsupervised Person Re-Identification</strong><br>　　intro: CVPR 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1803.09786">https://arxiv.org/abs/1803.09786</a></p><p><strong>Adaptation and Re-Identification Network: An Unsupervised Deep Transfer Learning Approach to Person Re-Identification</strong><br>　　intro: CVPR 2018 workshop. National Taiwan University &amp; Umbo Computer Vision<br>　　keywords: adaptation and re-identification network (ARN)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.09347">https://arxiv.org/abs/1804.09347</a></p><p><strong>Domain Adaptation through Synthesis for Unsupervised Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1804.10094">https://arxiv.org/abs/1804.10094</a></p><p><strong>Deep Association Learning for Unsupervised Video Person Re-identification</strong><br>　　intro: BMVC 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.07301">https://arxiv.org/abs/1808.07301</a></p><p><strong>Support Neighbor Loss for Person Re-Identification</strong><br>　　intro: ACM Multimedia (ACM MM) 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1808.06030">https://arxiv.org/abs/1808.06030</a></p><p><strong>Unsupervised Person Re-identification by Deep Learning Tracklet Association</strong><br>　　intro: ECCV 2018 Oral<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.02874">https://arxiv.org/abs/1809.02874</a></p><p><strong>Unsupervised Tracklet Person Re-Identification</strong><br>　　intro: TPAMI 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.00535">https://arxiv.org/abs/1903.00535</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/liminxian/DukeMTMC-SI-Tracklet">https://github.com/liminxian/DukeMTMC-SI-Tracklet</a></p><p><strong>Unsupervised Person Re-identification by Deep Asymmetric Metric Embedding</strong><br>　　intro: TPAMI<br>　　keywords: DEep Clustering-based Asymmetric MEtric Learning (DECAMEL)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.10177">https://arxiv.org/abs/1901.10177</a><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/KovenYu/DECAMEL">https://github.com/KovenYu/DECAMEL</a></p><p><strong>Unsupervised Person Re-identification by Soft Multilabel Learning</strong><br>　　intro: CVPR 2019 oral<br>　　intro: Sun Yat-sen University &amp; YouTu Lab &amp; Queen Mary University of London<br>　　keywords: MAR (MultilAbel Reference learning), soft multilabel-guided hard negative mining<br>　　project page: <a target="_blank" rel="noopener" href="https://kovenyu.com/publication/2019-cvpr-mar/">https://kovenyu.com/publication/2019-cvpr-mar/</a><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.06325">https://arxiv.org/abs/1903.06325</a><br>　　github(official, Pytorch): <a target="_blank" rel="noopener" href="https://github.com/KovenYu/MAR">https://github.com/KovenYu/MAR</a></p><p><strong>A Novel Unsupervised Camera-aware Domain Adaptation Framework for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03425">https://arxiv.org/abs/1904.03425</a></p><h2 id="Weakly-Supervised-Person-Re-identification"><a href="#Weakly-Supervised-Person-Re-identification" class="headerlink" title="Weakly Supervised Person Re-identification"></a>Weakly Supervised Person Re-identification</h2><p><strong>Weakly Supervised Person Re-Identification</strong><br>　　intro: CVPR 2019<br>　　keywords: multi-instance multi-label learning (MIML), Cross-View MIML (CV-MIML)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03832">https://arxiv.org/abs/1904.03832</a></p><p><strong>Weakly Supervised Person Re-identification: Cost-effective Learning with A New Benchmark</strong><br>　　keywords: SYSU-30k<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.03845">https://arxiv.org/abs/1904.03845</a></p><h2 id="Vehicle-Re-ID"><a href="#Vehicle-Re-ID" class="headerlink" title="Vehicle Re-ID"></a>Vehicle Re-ID</h2><p><strong>Learning Deep Neural Networks for Vehicle Re-ID with Visual-spatio-temporal Path Proposals</strong><br>　　intro: ICCV 2017<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1708.03918">https://arxiv.org/abs/1708.03918</a></p><p><strong>Viewpoint-Aware Attentive Multi-View Inference for Vehicle Re-Identification</strong><br>　　intro: CVPR 2018<br>　　paper: <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhou_Viewpoint-Aware_Attentive_Multi-View_CVPR_2018_paper.pdf</a></p><p><strong>RAM: A Region-Aware Deep Model for Vehicle Re-Identification</strong><br>　　intro: ICME 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1806.09283">https://arxiv.org/abs/1806.09283</a></p><p><strong>Vehicle Re-Identification in Context</strong><br>　　intro: Pattern Recognition - 40th German Conference, (GCPR) 2018, Stuttgart<br>　　project page: <a target="_blank" rel="noopener" href="https://qmul-vric.github.io/">https://qmul-vric.github.io/</a><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1809.09409">https://arxiv.org/abs/1809.09409</a></p><p><strong>Vehicle Re-identification Using Quadruple Directional Deep Learning Features</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1811.05163">https://arxiv.org/abs/1811.05163</a></p><p><strong>Coarse-to-fine: A RNN-based hierarchical attention model for vehicle re-identification</strong><br>　　intro: ACCV 2018<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1812.04239">https://arxiv.org/abs/1812.04239</a></p><p><strong>Vehicle Re-Identification: an Efficient Baseline Using Triplet Embedding</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1901.01015">https://arxiv.org/abs/1901.01015</a></p><p><strong>A Two-Stream Siamese Neural Network for Vehicle Re-Identification by Using Non-Overlapping Cameras</strong><br>　　intro: ICIP 2019<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1902.01496">https://arxiv.org/abs/1902.01496</a></p><p><strong>CityFlow: A City-Scale Benchmark for Multi-Target Multi-Camera Vehicle Tracking and Re-Identification</strong><br>　　intro: Accepted for oral presentation at CVPR 2019 with review ratings of 2 strong accepts and 1 accept (work done during an internship at NVIDIA)<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1903.09254">https://arxiv.org/abs/1903.09254</a></p><p><strong>Vehicle Re-identification in Aerial Imagery: Dataset and Approach</strong><br>　　intro: Northwestern Polytechnical University<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1904.01400">https://arxiv.org/abs/1904.01400</a></p><h2 id="Deep-Metric-Learning"><a href="#Deep-Metric-Learning" class="headerlink" title="Deep Metric Learning"></a>Deep Metric Learning</h2><p><strong>Deep Metric Learning for Person Re-Identification</strong><br>　　intro: ICPR 2014<br>　　paper: <a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf">http://www.cbsr.ia.ac.cn/users/zlei/papers/ICPR2014/Yi-ICPR-14.pdf</a></p><p><strong>Deep Metric Learning for Practical Person Re-Identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1407.4979">https://arxiv.org/abs/1407.4979</a></p><p><strong>Constrained Deep Metric Learning for Person Re-identification</strong><br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1511.07545">https://arxiv.org/abs/1511.07545</a></p><p><strong>Embedding Deep Metric for Person Re-identication A Study Against Large Variations</strong><br>　　intro: ECCV 2016<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1611.00137">https://arxiv.org/abs/1611.00137</a></p><p><strong>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities Transfer</strong><br>　　intro: TuSimple<br>　　keywords: pedestrian re-identification<br>　　arxiv: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1707.01220">https://arxiv.org/abs/1707.01220</a></p><h2 id="Projects"><a href="#Projects" class="headerlink" title="Projects"></a>Projects</h2><p><strong>Open-ReID: Open source person re-identification library in python</strong><br>　　intro: Open-ReID is a lightweight library of person re-identification for research purpose. It aims to provide a uniform interface for different datasets, a full set of models and evaluation metrics, as well as examples to reproduce (near) state-of-the-art results.<br>　　project page: <a target="_blank" rel="noopener" href="https://cysu.github.io/open-reid/">https://cysu.github.io/open-reid/</a><br>　　github(PyTorch): <a target="_blank" rel="noopener" href="https://github.com/Cysu/open-reid">https://github.com/Cysu/open-reid</a><br>　　examples: <a target="_blank" rel="noopener" href="https://cysu.github.io/open-reid/examples/training_id.html">https://cysu.github.io/open-reid/examples/training_id.html</a><br>　　benchmarks: <a target="_blank" rel="noopener" href="https://cysu.github.io/open-reid/examples/benchmarks.html">https://cysu.github.io/open-reid/examples/benchmarks.html</a></p><p><strong>caffe-PersonReID</strong><br>　　intro: Person Re-Identification: Multi-Task Deep CNN with Triplet Loss<br>　　gtihub: <a target="_blank" rel="noopener" href="https://github.com/agjayant/caffe-Person-ReID">https://github.com/agjayant/caffe-Person-ReID</a></p><p><strong>Person_reID_baseline_pytorch</strong><br>　　intro: Pytorch implement of Person re-identification baseline<br>　　arxiv: <a target="_blank" rel="noopener" href="https://github.com/layumi/Person_reID_baseline_pytorch">https://github.com/layumi/Person_reID_baseline_pytorch</a></p><p><strong>deep-person-reid</strong><br>　　intro: Pytorch implementation of deep person re-identification models.<br>　　github: <a target="_blank" rel="noopener" href="https://github.com/KaiyangZhou/deep-person-reid">https://github.com/KaiyangZhou/deep-person-reid</a></p><p><strong>ReID_baseline</strong><br>　　intro: Baseline model (with bottleneck) for person ReID (using softmax and triplet loss).<br>　　github: <a target="_blank" rel="noopener" href="https://github.com/L1aoXingyu/reid_baseline">https://github.com/L1aoXingyu/reid_baseline</a><br>　　blog: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/40514536">https://zhuanlan.zhihu.com/p/40514536</a></p><p><strong>gluon-reid</strong><br>　　intro: A code gallery for person re-identification with mxnet-gluon, and I will reproduce many STOA algorithm.<br>　　github: <a target="_blank" rel="noopener" href="https://github.com/xiaolai-sqlai/gluon-reid">https://github.com/xiaolai-sqlai/gluon-reid</a></p><h2 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h2><p><strong>DukeMTMC-reID</strong><br>　　intro: The Person re-ID Evaluation Code for DukeMTMC-reID Dataset (Including Dataset Download)<br>　　github: <a target="_blank" rel="noopener" href="https://github.com/layumi/DukeMTMC-reID_evaluation">https://github.com/layumi/DukeMTMC-reID_evaluation</a></p><p><strong>DukeMTMC-reID_baseline (Matlab)</strong><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/layumi/DukeMTMC-reID_baseline">https://github.com/layumi/DukeMTMC-reID_baseline</a></p><p><strong>Code for IDE baseline on Market-1501</strong><br>　　github: <a target="_blank" rel="noopener" href="https://github.com/zhunzhong07/IDE-baseline-Market-1501">https://github.com/zhunzhong07/IDE-baseline-Market-1501</a></p><h2 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h2><p><strong>Re-ID 数据集汇总</strong><br><a target="_blank" rel="noopener" href="https://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html">https://robustsystems.coe.neu.edu/sites/robustsystems.coe.neu.edu/files/systems/projectpages/reiddataset.html</a></p><p><strong>Attribute相关数据集</strong><br>RAP: <a target="_blank" rel="noopener" href="http://rap.idealtest.org/">http://rap.idealtest.org/</a><br>Attribute for Market-1501 and DukeMTMC_reID: <a target="_blank" rel="noopener" href="https://vana77.github.io/">https://vana77.github.io/</a></p><p><strong>视频相关数据集</strong><br>Mars: <a target="_blank" rel="noopener" href="http://liangzheng.org/Project/project_mars.html">http://liangzheng.org/Project/project_mars.html</a><br>PRID2011: <a target="_blank" rel="noopener" href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/">https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/</a></p><p><strong>NLP相关数据集</strong><br>自然语言搜图像: <a target="_blank" rel="noopener" href="http://xiaotong.me/static/projects/person-search-language/dataset.html">http://xiaotong.me/static/projects/person-search-language/dataset.html</a><br>自然语言搜行人所在视频: <a target="_blank" rel="noopener" href="http://www.mi.t.u-tokyo.ac.jp/projects/person_search">http://www.mi.t.u-tokyo.ac.jp/projects/person_search</a></p><h2 id="Tutorials"><a href="#Tutorials" class="headerlink" title="Tutorials"></a>Tutorials</h2><p><strong>1st Workshop on Target Re-Identification and Multi-Target Multi-Camera Tracking</strong><br><a target="_blank" rel="noopener" href="https://reid-mct.github.io/">https://reid-mct.github.io/</a></p><p><strong>Target Re-Identification and Multi-Target Multi-Camera Tracking</strong><br><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py">http://openaccess.thecvf.com/CVPR2017_workshops/CVPR2017_W17.py</a></p><p><strong>Person Re-Identification: Theory and Best Practice</strong><br><a target="_blank" rel="noopener" href="http://www.micc.unifi.it/reid-tutorial/">http://www.micc.unifi.it/reid-tutorial/</a></p><h2 id="Experts"><a href="#Experts" class="headerlink" title="Experts"></a>Experts</h2><blockquote><p>Listed in No Particular Order</p></blockquote><ul><li><strong>Shaogang Gong</strong> - [<a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~sgg/]">http://www.eecs.qmul.ac.uk/~sgg/]</a></li><li><strong>Xiaogang Wang</strong> - [<a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~xgwang/]">http://www.ee.cuhk.edu.hk/~xgwang/]</a></li><li><strong>Weishi Zheng</strong> - [<a target="_blank" rel="noopener" href="http://isee.sysu.edu.cn/~zhwshi/]">http://isee.sysu.edu.cn/~zhwshi/]</a></li><li><strong>Liang Zheng</strong> - [<a target="_blank" rel="noopener" href="http://www.liangzheng.com.cn/]">http://www.liangzheng.com.cn/]</a></li><li><strong>Li Zhang</strong> - [<a target="_blank" rel="noopener" href="http://www.robots.ox.ac.uk/~lz/]">http://www.robots.ox.ac.uk/~lz/]</a></li><li><strong>Xiatian Zhu</strong> - [<a target="_blank" rel="noopener" href="http://www.eecs.qmul.ac.uk/~xiatian/index.html]">http://www.eecs.qmul.ac.uk/~xiatian/index.html]</a></li><li><strong>Chen Change Loy</strong> - [<a target="_blank" rel="noopener" href="https://staff.ie.cuhk.edu.hk/~ccloy/]">https://staff.ie.cuhk.edu.hk/~ccloy/]</a></li><li><strong>Qi Tian</strong> - [<a target="_blank" rel="noopener" href="http://www.cs.utsa.edu/~qitian/tian-publication-year.html]">http://www.cs.utsa.edu/~qitian/tian-publication-year.html]</a></li><li><strong>Shengcai Liao</strong> - [<a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/users/scliao/]">http://www.cbsr.ia.ac.cn/users/scliao/]</a></li><li><strong>Rui Zhao</strong> - [<a target="_blank" rel="noopener" href="http://www.ee.cuhk.edu.hk/~rzhao/]">http://www.ee.cuhk.edu.hk/~rzhao/]</a></li><li><strong>Yang Yang</strong> - [<a target="_blank" rel="noopener" href="http://www.cbsr.ia.ac.cn/users/yyang/main.htm]">http://www.cbsr.ia.ac.cn/users/yyang/main.htm]</a></li><li><strong>Ling Shao</strong> - [<a href="http://lshao.staff.shef.ac.uk]">http://lshao.staff.shef.ac.uk]</a></li><li><strong>Ziyan Wu</strong> - [<a target="_blank" rel="noopener" href="http://wuziyan.com/]">http://wuziyan.com/]</a></li><li><strong>DaPeng Chen</strong> - [<a target="_blank" rel="noopener" href="http://gr.xjtu.edu.cn/web/dapengchen/home]">http://gr.xjtu.edu.cn/web/dapengchen/home]</a></li><li><strong>Horst Bischof</strong> - [<a target="_blank" rel="noopener" href="https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/prid450s]">https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/prid450s]</a></li><li><strong>Niki Martinel</strong> - [<a target="_blank" rel="noopener" href="http://users.dimi.uniud.it/~niki.martinel/]">http://users.dimi.uniud.it/~niki.martinel/]</a></li><li><strong>Liang Lin</strong> - [<a target="_blank" rel="noopener" href="http://hcp.sysu.edu.cn/home/]">http://hcp.sysu.edu.cn/home/]</a></li><li><strong>Le An</strong> - [<a target="_blank" rel="noopener" href="http://auto.hust.edu.cn/index.php?a=shows&amp;catid=28&amp;id=134]">http://auto.hust.edu.cn/index.php?a=shows&amp;catid=28&amp;id=134]</a></li><li><strong>Xiang Bai</strong> - [<a target="_blank" rel="noopener" href="http://mc.eistar.net/~xbai/index.html]">http://mc.eistar.net/~xbai/index.html]</a></li><li><strong>Xiaoyuan Jing</strong> - [<a target="_blank" rel="noopener" href="http://mla.whu.edu.cn/plus/list.php?tid=2]">http://mla.whu.edu.cn/plus/list.php?tid=2]</a></li><li><strong>Fei Xiong</strong> - [<a target="_blank" rel="noopener" href="http://robustsystems.coe.neu.edu/?q=content/research]">http://robustsystems.coe.neu.edu/?q=content/research]</a></li><li><strong>DaPeng Chen</strong> - [<a target="_blank" rel="noopener" href="http://gr.xjtu.edu.cn/web/dapengchen/home]">http://gr.xjtu.edu.cn/web/dapengchen/home]</a></li><li><strong>Zhedong Zheng</strong> - [<a target="_blank" rel="noopener" href="http://zdzheng.xyz/]">http://zdzheng.xyz/]</a></li><li><strong>Zhun Zhong</strong> - [<a target="_blank" rel="noopener" href="http://zhunzhong.site/]">http://zhunzhong.site/]</a></li></ul><h2 id="Resources"><a href="#Resources" class="headerlink" title="Resources"></a>Resources</h2><p><strong>Re-id Resources</strong><br><a target="_blank" rel="noopener" href="https://wangzwhu.github.io/home/re_id_resources.html">https://wangzwhu.github.io/home/re_id_resources.html</a></p><p><strong>Zhuanzhi</strong><br><a target="_blank" rel="noopener" href="http://www.zhuanzhi.ai/topic/2001183057160970">http://www.zhuanzhi.ai/topic/2001183057160970</a></p><p><strong>Zhihu</strong><br>行人重识别: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/personReid">https://zhuanlan.zhihu.com/personReid</a><br>Person Re-id: <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/re-id">https://zhuanlan.zhihu.com/re-id</a><br>Topci: <a target="_blank" rel="noopener" href="https://www.zhihu.com/topic/20087378/hot">https://www.zhihu.com/topic/20087378/hot</a></p><p><strong>Blogs</strong><br>行人重识别简介: <a target="_blank" rel="noopener" href="https://www.jianshu.com/p/98cc04cca0ae">https://www.jianshu.com/p/98cc04cca0ae</a><br>基于深度学习的Person Re-ID（综述）: <a target="_blank" rel="noopener" href="https://blog.csdn.net/linolzhang/article/details/71075756">https://blog.csdn.net/linolzhang/article/details/71075756</a><br>行人再识别（行人重识别）【包含与行人检测的对比】: <a target="_blank" rel="noopener" href="https://blog.csdn.net/liuqinglong110/article/details/41699861">https://blog.csdn.net/liuqinglong110/article/details/41699861</a><br>行人重识别综述（Person Re-identification: Past, Present and Future）: <a target="_blank" rel="noopener" href="https://blog.csdn.net/auto1993/article/details/74091803">https://blog.csdn.net/auto1993/article/details/74091803</a><br>行人重识别: <a target="_blank" rel="noopener" href="http://cweihang.cn/ml/reid/">http://cweihang.cn/ml/reid/</a></p></div><div class="article-licensing box"><div class="licensing-title"><p>行人重识别Person Re-identification总结</p><p><a href="http://blog.fcj.one/reid-overview.html">http://blog.fcj.one/reid-overview.html</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>ChangingFond</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2018-07-14</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2022-04-29</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><div class="article-tags is-size-7 mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Deep-Learning/">Deep Learning</a><a class="link-muted mr-2" rel="tag" href="/tags/Person-Re-ID/">Person Re-ID</a></div></article></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/hexo-custom-page.html"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Hexo博客跳过渲染，创建自定义网页</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/reid-market-1501.html"><span class="level-item">Person Re-identification数据集描述——Market-1501</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content" id="valine-thread"></div><script src="//cdn.jsdelivr.net/npm/leancloud-storage@3/dist/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine@1.4.16/dist/Valine.min.js"></script><script>new Valine({el:"#valine-thread",appId:"aW6XlFUNratBmLW8CfW2jqhS-gzGzoHsz",appKey:"7tjG9kHG4QIM8m9JjDBzVoFq",avatar:"mm",avatarForce:!1,meta:["nick","mail","link"],pageSize:10,lang:"zh-CN",visitor:!1,highlight:!0,recordIP:!1,enableQQ:!1,requiredFields:[]})</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-4-widescreen order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar" src="/img/avatar.png" alt="ChangingFond"></figure><p class="title is-size-4 is-block" style="line-height:inherit">ChangingFond</p><p class="is-size-6 is-block">fcj1021@hotmail.com</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Beijing, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">40</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">6</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">25</p></a></div></div></nav></div></div><div class="card widget" data-type="recent-posts"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><div class="media-content"><p class="date"><time datetime="2022-03-03T20:15:50.000Z">2022-03-03</time></p><p class="title"><a href="/guava-primitives-Ints.html">Guava 源码阅读系列——Primitives Ints 类</a></p><p class="categories"><a href="/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/">源码阅读</a> / <a href="/categories/%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB/Guava/">Guava</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2020-10-28T23:32:43.000Z">2020-10-28</time></p><p class="title"><a href="/mockito-zh-doc.html">Mockito 3.6.0 中文文档</a></p><p class="categories"><a href="/categories/%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/">编程笔记</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2019-11-19T12:58:24.000Z">2019-11-19</time></p><p class="title"><a href="/apache-https.html">利用 Let&#039;s Encrypt 配置 Apache SSL 证书</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%94%9F/">技术人生</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2019-10-22T20:16:01.000Z">2019-10-22</time></p><p class="title"><a href="/hexo-travis.html">利用 Travis CI 自动部署 Hexo 博客最佳实践</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%94%9F/">技术人生</a></p></div></article><article class="media"><div class="media-content"><p class="date"><time datetime="2019-07-17T14:32:12.000Z">2019-07-17</time></p><p class="title"><a href="/hexo-gulp-post.html">Hexo 博客静态资源压缩</a></p><p class="categories"><a href="/categories/%E6%8A%80%E6%9C%AF%E4%BA%BA%E7%94%9F/">技术人生</a></p></div></article></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/C/"><span class="tag">C#</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Cuda/"><span class="tag">Cuda</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Deep-Learning/"><span class="tag">Deep Learning</span><span class="tag">9</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Elasticsearch/"><span class="tag">Elasticsearch</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Frontend/"><span class="tag">Frontend</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Git/"><span class="tag">Git</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Guava/"><span class="tag">Guava</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Java/"><span class="tag">Java</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/JavaScript/"><span class="tag">JavaScript</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Linux/"><span class="tag">Linux</span><span class="tag">12</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Markdown/"><span class="tag">Markdown</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Mockito/"><span class="tag">Mockito</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Person-Re-ID/"><span class="tag">Person Re-ID</span><span class="tag">5</span></a></div><div class="control"><a class="tags has-addons" href="/tags/ResNet/"><span class="tag">ResNet</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL/"><span class="tag">SQL</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/SQL-Server/"><span class="tag">SQL Server</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Windows/"><span class="tag">Windows</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/apache/"><span class="tag">apache</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/css/"><span class="tag">css</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/git/"><span class="tag">git</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/php/"><span class="tag">php</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/python/"><span class="tag">python</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/vps/"><span class="tag">vps</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E8%AE%BA%E6%96%87/"><span class="tag">论文</span><span class="tag">1</span></a></div></div></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/avatar.png" alt="ChangingFond" height="28"></a><p class="is-size-7"><span>&copy; 2022 ChangingFond</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ChangingFond"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer="defer"></script><script>moment.locale("zh-CN")</script><script>var IcarusThemeSettings={article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer="defer"></script><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer="defer"></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer="defer"></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer="defer"></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.9/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer="defer"></script><script src="/js/main.js" defer="defer"></script><script src="/js/custom.js" defer="defer"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer="defer"></script><script>document.addEventListener("DOMContentLoaded",function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})})</script></body></html>